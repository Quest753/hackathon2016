{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import hinge_loss\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"githubUsersFull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('hhImprovedCleared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = all_data.drop(\"Salary\",axis = 1).as_matrix()\n",
    "y = np.log(all_data['Salary'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"githubUsersFull.csv\")\n",
    "c = pd.read_csv('sending_value.csv')\n",
    "for i in new_data.columns:\n",
    "    if i not in c['data'].values:\n",
    "        new_data[i] = np.zeros(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = PCA(n_components=3)\n",
    "X = a.fit_transform(X)\n",
    "ans = a.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = SVR().fit(Xtrain,Ytrain)\n",
    "answer = z.predict(ans)\n",
    "answer = pd.DataFrame(answer,columns = ['Salay'])\n",
    "answer.to_csv('Salay.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1.9K Dec 11 14:17 pca.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lht pca.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(z, f)\n",
    "with open('pca.pickle', 'wb') as f:\n",
    "    pickle.dump(a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_x = T.matrix('one of user',dtype = 'float32')\n",
    "target_y = T.matrix('answer',dtype = 'float32')\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape= (None, 3), input_var = input_x)\n",
    "\n",
    "dense = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=50,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "W=lasagne.init.GlorotUniform())\n",
    "dense = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=20,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "W=lasagne.init.GlorotUniform())\n",
    "out = lasagne.layers.DenseLayer(dense,num_units=1,nonlinearity = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = lasagne.layers.get_output(out)\n",
    "loss = lasagne.objectives.squared_error(predict,target_y).mean()\n",
    "weight = lasagne.layers.get_all_params(out,trainable=True)\n",
    "updates = lasagne.updates.adam(loss,weight,learning_rate = 0.01)\n",
    "train_fun = theano.function([input_x,target_y],loss,updates = updates,name='high')\n",
    "accuarcy_fun = theano.function([input_x,target_y],loss)\n",
    "predict_fun = theano.function([input_x],predict)\n",
    "test = []\n",
    "train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict.eval({input_x: X[:10].values.astype(np.float32)}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss.eval({input_x: X[:10].values.astype(np.float32), target_y: y[:10].reshape((-1, 1)).astype(np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize):\n",
    "    assert len(inputs) == len(targets)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 100 #количество проходов по данным\n",
    "train_r = []\n",
    "val_r = []\n",
    "\n",
    "batch_size = 50 #размер мини-батча\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Xtrain, Ytrain,batch_size):\n",
    "        inputs, target = batch\n",
    "        train_err_batch= train_fun(inputs.astype(\"float32\"), target.reshape((-1,1)).astype('float32'))\n",
    "        train_err += train_err_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(Xtest, Ytest, batch_size):\n",
    "        inputs, target = batch\n",
    "        val_acc += accuarcy_fun(inputs.astype(\"float32\"), target.reshape((-1,1)).astype('float32'))\n",
    "    \n",
    "    \n",
    "#     # Then we print the results for this epoch:\n",
    "#     print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "#         epoch + 1, num_epochs, time.time() - start_time))\n",
    "    train_r.append(train_err/train_batches)\n",
    "    val_r.append(np.mean(val_acc))\n",
    "#    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(np.mean(train_err/train_batches)))\n",
    "#     print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "#         train_acc / train_batches * 100))\n",
    "#     train.append(train_acc / train_batches * 100)\n",
    "#    print(\"  validation loss:\\t\\t{:.2f}\".format(\n",
    "#        np.mean(val_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(val_r)\n",
    "plt.show()\n",
    "plt.plot(train_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
